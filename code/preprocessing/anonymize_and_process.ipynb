{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c911c2b-9c17-4429-a2a8-7eaae5f3c270",
   "metadata": {},
   "source": [
    "## ChatGPT Transcripts Processing\n",
    "This notebook processes a collection of `.txt` files located in the specified directory (`/Users/mts517/Desktop/NLP Analysis/TXT/`). Each `.txt` file follows the naming format `NetID_Ex#.txt`, where `NetID` is a unique identifier, and `Ex#` denotes an exercise number.\n",
    "\n",
    "The script performs the following tasks for each `.txt` file:\n",
    "1. Extracts the `NetID` and integer portion of the `Exercise` number from the filename.\n",
    "2. Tokenizes the file content using \"You\" and \"ChatGPT\" as delimiters to identify each interaction.\n",
    "3. Captures the student's prompt following \"You\" and ChatGPT's response following \"ChatGPT.\"\n",
    "4. Writes each interaction to a CSV file (`ChatGPT_Scripts.csv`) located at `/Users/mts517/Desktop/NLP Analysis/`, with the following columns:\n",
    "   - **NetID**: Identifier from the filename.\n",
    "   - **Exercise**: The integer exercise number.\n",
    "   - **Student_prompt**: Text from the student's input.\n",
    "   - **ChatGPT_answer**: Text from ChatGPT's response.\n",
    "   - **Interaction_Sequence**: The sequence number of the interaction within the file.\n",
    "\n",
    "The notebook continues this process until all `.txt` files in the directory are processed, resulting in a structured CSV file with each interaction organized sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffd75d59-25d9-44b9-a955-f9abab57fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Define the directory path and output CSV file path\n",
    "directory_path = \"/Users/mts517/Desktop/NLP Analysis/TXT/\"\n",
    "output_csv_path = \"/Users/mts517/Desktop/NLP Analysis/ChatGPT_Scripts.csv\"\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"NetID\", \"Exercise\", \"Student_prompt\", \"ChatGPT_answer\", \"Interaction_Sequence\"])\n",
    "    \n",
    "    # Iterate over each .txt file in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Extract NetID and Exercise from the filename (format: NetID_Ex#.txt)\n",
    "            netid = filename.split(\"_\")[0]\n",
    "            exercise_match = re.search(r'\\d+', filename.split(\"_\")[1])  # Find integers in the exercise part\n",
    "            exercise = int(exercise_match.group()) if exercise_match else None\n",
    "            \n",
    "            # Read the contents of the .txt file\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "                # Initialize variables to store prompts and answers\n",
    "                student_prompt, chatgpt_answer = \"\", \"\"\n",
    "                interaction_sequence = 1\n",
    "                in_student_prompt = False\n",
    "                in_chatgpt_answer = False\n",
    "                \n",
    "                # Process each line to capture interactions\n",
    "                for line in lines:\n",
    "                    line = line.strip()  # Remove whitespace characters\n",
    "                    \n",
    "                    # Check for \"You\" or \"ChatGPT\" delimiters\n",
    "                    if line == \"You\":\n",
    "                        # Save previous interaction if any\n",
    "                        if student_prompt and chatgpt_answer:\n",
    "                            writer.writerow([netid, exercise, student_prompt.strip(), chatgpt_answer.strip(), interaction_sequence])\n",
    "                            interaction_sequence += 1\n",
    "                            student_prompt, chatgpt_answer = \"\", \"\"\n",
    "                        # Start capturing new Student prompt\n",
    "                        in_student_prompt = True\n",
    "                        in_chatgpt_answer = False\n",
    "                    elif line == \"ChatGPT\":\n",
    "                        # Start capturing ChatGPT answer\n",
    "                        in_student_prompt = False\n",
    "                        in_chatgpt_answer = True\n",
    "                    else:\n",
    "                        # Append lines to respective prompts or answers\n",
    "                        if in_student_prompt:\n",
    "                            student_prompt += \" \" + line\n",
    "                        elif in_chatgpt_answer:\n",
    "                            chatgpt_answer += \" \" + line\n",
    "                \n",
    "                # Write any last interaction to the CSV (if exists)\n",
    "                if student_prompt and chatgpt_answer:\n",
    "                    writer.writerow([netid, exercise, student_prompt.strip(), chatgpt_answer.strip(), interaction_sequence])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c81080-7c01-43ac-8d3e-5e24cf280a48",
   "metadata": {},
   "source": [
    "# Joining the two datasets\n",
    "\n",
    "This script performs a left join operation on two CSV datasets located on the user's desktop, specifically targeting columns `NetID` and `Exercise`. The purpose is to merge data from the `ChatGPT_Scripts.csv` file and `Final_Eye_Tracking_Profiles_Merged.csv` file based on shared identifiers in `NetID` and `Exercise`. \n",
    "\n",
    "1. **Paths**: The paths are defined for input files (`ChatGPT_Scripts.csv` and `Final_Eye_Tracking_Profiles_Merged.csv`) and the output file (`Joined_Dataset.csv`).\n",
    "2. **Loading Datasets**: The script reads both CSV files into pandas DataFrames.\n",
    "3. **Left Join Operation**: A left join is performed on `NetID` and `Exercise` columns, resulting in a merged DataFrame that retains all rows from `ChatGPT_Scripts.csv` and adds matching rows from `Final_Eye_Tracking_Profiles_Merged.csv` where `NetID` and `Exercise` values match.\n",
    "4. **Column Reordering**: After the merge, columns `Student_prompt`, `ChatGPT_answer`, and `Interaction_Sequence` are moved to the end of the DataFrame to ensure these specific columns are the last in the dataset.\n",
    "5. **Saving the Output**: The final DataFrame is saved as `Joined_Dataset.csv` in the specified directory.\n",
    "\n",
    "This merged dataset provides a comprehensive view of data from both sources, with prompt-answer interaction sequences organized at the end of each row for streamlined analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39267a1-41f7-4e25-b629-de53eb5bbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "chatgpt_scripts_path = \"/Users/mts517/Desktop/NLP Analysis/ChatGPT_Scripts.csv\"\n",
    "eye_tracking_profiles_path = \"/Users/mts517/Desktop/NLP Analysis/Final_Eye_Tracking_Profiles_Merged.csv\"\n",
    "output_path = \"/Users/mts517/Desktop/NLP Analysis/Joined_Dataset.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "chatgpt_df = pd.read_csv(chatgpt_scripts_path)\n",
    "eye_tracking_df = pd.read_csv(eye_tracking_profiles_path)\n",
    "\n",
    "# Perform a left join on 'NetID' and 'Exercise' columns\n",
    "merged_df = pd.merge(chatgpt_df, eye_tracking_df, on=['NetID', 'Exercise'], how='left')\n",
    "\n",
    "# Move 'Student_prompt', 'ChatGPT_answer', and 'Interaction_Sequence' to the end of the DataFrame\n",
    "columns_order = [col for col in merged_df.columns if col not in ['Student_prompt', 'ChatGPT_answer', 'Interaction_Sequence']]\n",
    "columns_order += ['Student_prompt', 'ChatGPT_answer', 'Interaction_Sequence']\n",
    "merged_df = merged_df[columns_order]\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85efba5-aa97-48e3-a71e-85cc94ec18e8",
   "metadata": {},
   "source": [
    "# NetID Anonymization and filling the NaN values\n",
    "\n",
    "This script anonymizes the `NetID` column in the joined dataset by replacing each unique `NetID` with a unique integer identifier, ensuring data privacy. It also fills any missing values in the dataset with `NaN`.\n",
    "\n",
    "1. **File Paths**: Specifies the file path for the input dataset (`Joined_Dataset.csv`) and the output path for the anonymized dataset (`Anonymized_Joined_Dataset.csv`).\n",
    "2. **Load Dataset**: Reads the joined dataset into a pandas DataFrame.\n",
    "3. **Generate Anonymized Mapping**: Creates a mapping dictionary that assigns a unique integer to each distinct `NetID`, starting from 1.\n",
    "4. **Anonymize `NetID`**: Replaces the original `NetID` values in the DataFrame with their respective integer identifiers from the mapping dictionary.\n",
    "5. **Fill Missing Values**: Replaces any missing values across the DataFrame with `NaN` to ensure data completeness and consistency.\n",
    "6. **Save Output**: Exports the anonymized and cleaned dataset as `Anonymized_Joined_Dataset.csv`.\n",
    "\n",
    "The result is a dataset where `NetID` values are anonymized, and missing entries are marked as `NaN`, supporting privacy and data integrity for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d01067-dfff-4ff4-bc0c-16adef886c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define file path for the joined dataset\n",
    "joined_dataset_path = \"/Users/mts517/Desktop/NLP Analysis/Joined_Dataset.csv\"\n",
    "anonymized_output_path = \"/Users/mts517/Desktop/NLP Analysis/Anonymized_Joined_Dataset.csv\"\n",
    "\n",
    "# Load the joined dataset\n",
    "df = pd.read_csv(joined_dataset_path)\n",
    "\n",
    "# Create a unique integer mapping for each unique NetID\n",
    "netid_mapping = {netid: i for i, netid in enumerate(df['NetID'].unique(), start=1)}\n",
    "\n",
    "# Replace NetID values with the corresponding integer\n",
    "df['NetID'] = df['NetID'].map(netid_mapping)\n",
    "\n",
    "# Fill missing values with NaN\n",
    "df.fillna(np.nan, inplace=True)\n",
    "\n",
    "# Save the anonymized dataset to a new CSV file\n",
    "df.to_csv(anonymized_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
